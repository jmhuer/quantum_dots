{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled12.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP2CpJDtRCRqo8OfviVxVi7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmhuer/quantum_dots/blob/main/quantum_dots_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyiZYk_GcXQE"
      },
      "source": [
        "# Load data from excell\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFF6ShtM2arX",
        "outputId": "9a058d10-7bd6-4bf6-af21-607d0d916efc"
      },
      "source": [
        "!pip install skorch\n",
        "!pip install tqdm\n",
        "!git clone https://github.com/jmhuer/optimization_tools\n",
        "\n",
        "from optimization_tools.utils import download_gdrive\n",
        "data = '14mee8d0GDbwNzIprVSJoWdojGtqO_aTX' ##google drive id of excell \n",
        "download_gdrive(data)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting skorch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/bd/f714a726f2f3106abe5a24d11b1fe8e20570323479164b829f5d4b80f7f2/skorch-0.10.0-py3-none-any.whl (128kB)\n",
            "\r\u001b[K     |██▌                             | 10kB 17.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 23.4MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 30kB 15.9MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 40kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 51kB 16.7MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 61kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 71kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 81kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 92kB 12.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 102kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 112kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 122kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 13.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.7/dist-packages (from skorch) (4.41.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from skorch) (0.8.9)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from skorch) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from skorch) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from skorch) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->skorch) (1.0.1)\n",
            "Installing collected packages: skorch\n",
            "Successfully installed skorch-0.10.0\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
            "Cloning into 'optimization_tools'...\n",
            "remote: Enumerating objects: 46, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 46 (delta 12), reused 39 (delta 8), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (46/46), done.\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=14mee8d0GDbwNzIprVSJoWdojGtqO_aTX\n",
            "To: /content/QDots data - Juan.xlsx\n",
            "100%|##########| 39.7k/39.7k [00:00<00:00, 24.4MB/s]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHStJWQsbRg1"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def open_excel(filename):\n",
        "    excell = pd.ExcelFile(filename)\n",
        "    excell.sheet_names\n",
        "    df = excell.parse(\"Sheet1\")\n",
        "    df.columns = df.columns.map(str)\n",
        "    df = df.dropna().reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "df = open_excel(\"/content/QDots data - Juan.xlsx\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQd7hHq4qytX"
      },
      "source": [
        "## Pre-processing + model defenitions before training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBw-MgBYqyF1"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "X_columns  = [\"415\",\"445\",\"480\", \"515\", \"555\", \"590\", \"630\", \"680\"]\n",
        "Y1_columns = [\"# emitters λ1 (x1010)\"]\n",
        "Y2_columns = [\"# emitters λ2 (x1010)\"]\n",
        "Y_columns  = [\"# emitters λ1 (x1010)\", \"# emitters λ2 (x1010)\"]\n",
        "\n",
        "##LINEAR REGRESSION\n",
        "lr_reg = make_pipeline(LinearRegression())\n",
        "\n",
        "##RandomForestRegressor\n",
        "rf_reg = make_pipeline(StandardScaler(), RandomForestRegressor())\n",
        "parameters = {'randomforestregressor__max_depth':[1,2,3,4,5]}\n",
        "rf_reg = GridSearchCV(rf_reg, parameters)\n",
        "  \n",
        "##SVM\n",
        "svr = make_pipeline(StandardScaler(), SVR(kernel='linear'))\n",
        "parameters = {'svr__kernel':['linear', 'rbf'], 'svr__epsilon':[0.1, 0.2, 0.3,0.6], 'svr__C': [0.1,0.5,1,2,3]}\n",
        "svr = GridSearchCV(svr, parameters)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbVTXkEnydv3"
      },
      "source": [
        "# NN stuff here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY5agDS6ydI7"
      },
      "source": [
        "from torch import nn\n",
        "from skorch import NeuralNetRegressor\n",
        "\n",
        "class MultivariateLinearRegression(nn.Module):\n",
        "    def __init__(self, input = 8, num_units=10, nonlin=nn.LeakyReLU()):\n",
        "          super(MultivariateLinearRegression, self).__init__()\n",
        "          self.dense0 = nn.Linear(input, num_units)\n",
        "          self.nonlin = nonlin\n",
        "          self.dropout = nn.Dropout(0.1)\n",
        "          self.output = nn.Linear(num_units, 1)\n",
        "    def forward(self, X, **kwargs):\n",
        "          X = self.nonlin(self.dense0(X))\n",
        "          X = self.dropout(X)\n",
        "          X = self.output(X)\n",
        "          return X\n",
        "\n",
        "net = NeuralNetRegressor(\n",
        "    MultivariateLinearRegression().double(),\n",
        "    iterator_train__shuffle = False,\n",
        "    train_split = False,\n",
        "    verbose = 0)\n",
        "\n",
        "myNN_reg = make_pipeline(StandardScaler(), net)\n",
        "\n",
        "params = {\n",
        "    'neuralnetregressor__lr': [0.001, 0.002, 0.003],\n",
        "    'neuralnetregressor__max_epochs': [10, 20, 30]\n",
        "}\n",
        "myNN_reg = GridSearchCV(myNN_reg, params)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0PMkxpX5zOk"
      },
      "source": [
        "##Training loop w/ cross-validation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkC4fqXe5yVG",
        "outputId": "45609b23-61af-4025-c5e4-45067c6bbdeb"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "##function to help store history and metrics of each cross validation run\n",
        "history = dict()\n",
        "def append_history(history, algorithm, name, X_test, Y_test):\n",
        "    #calculate performance metrics\n",
        "    Y_pred = algorithm.predict(X_test)\n",
        "    score = algorithm.score(X_test, Y_test)\n",
        "    #if dict does not contain current alg, create a key\n",
        "    if name not in history.keys(): \n",
        "        history[name] = {\"mse\": [], \"r2\": [], \"predictions\": [], \"actual\": []}\n",
        "    #store preformance metrics & history\n",
        "    history[name][\"mse\"].append(mean_squared_error(Y_test, Y_pred))\n",
        "    history[name][\"r2\"].append(score)\n",
        "    history[name][\"predictions\"].append(Y_pred)\n",
        "    history[name][\"actual\"].append(Y_test)\n",
        "    return history\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##For Cross validations \n",
        "kf = KFold(n_splits=10, random_state=42, shuffle=True) # Define the split - into 10 folds \n",
        "\n",
        "##A progress bar because Im fancy\n",
        "pbar = tqdm(total=100,position=0, leave=True)\n",
        "\n",
        "##MAIN LOOP\n",
        "for train_index, test_index in kf.split(df):\n",
        "    X_train, X_test = df.loc[train_index, X_columns], df.loc[test_index, X_columns]\n",
        "    Y_train, Y_test = df.loc[train_index, Y1_columns], df.loc[test_index, Y1_columns]\n",
        "\n",
        "    ##LINEAR REGRESSION\n",
        "    lr_reg.fit(X_train, Y_train.values.ravel())\n",
        "    history = append_history(history, lr_reg, \"linear_regression\", X_test, Y_test)\n",
        "\n",
        "    ##RandomForestRegressor\n",
        "    rf_reg.fit(X_train, Y_train.values.ravel())\n",
        "    history = append_history(history, rf_reg, \"rf_reg\", X_test, Y_test)\n",
        "\n",
        "    ##Support Vecotor Regressor \n",
        "    svr.fit(X_train, Y_train.values.ravel())\n",
        "    history = append_history(history, svr, \"svr\", X_test, Y_test)\n",
        "\n",
        "    # ##NN Regressor\n",
        "    myNN_reg.fit(X_train, Y_train.values)\n",
        "    history = append_history(history, myNN_reg, \"myNN_reg\", X_test, Y_test)\n",
        "\n",
        "\n",
        "    pbar.update(10)\n",
        "pbar.close()\n",
        "\n",
        "print('\\n')\n",
        "for i in history.keys():\n",
        "    print('\\n', i,\" : \", sum(history[i][\"mse\"]) / len(history[i][\"mse\"]))\n",
        "    print(\"---\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [01:27<00:00,  1.14it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " linear_regression  :  4.050225300323952\n",
            "---\n",
            "\n",
            " rf_reg  :  0.7655509547828852\n",
            "---\n",
            "\n",
            " svr  :  4.476981793012247\n",
            "---\n",
            "\n",
            " myNN_reg  :  14.863602567054102\n",
            "---\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xg_ANKQ0JCa"
      },
      "source": [
        "# Ploting utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GudKpHXc0MBg"
      },
      "source": [
        "import plotly.graph_objects as graph\n",
        "def plot(all_history:list, title:str, log = False):\n",
        "    \"\"\"\n",
        "    input:\n",
        "        all_history: list of dicts to plot\n",
        "    ret:\n",
        "        None: show plotly fig\n",
        "    \"\"\"\n",
        "    fig = graph.Figure(layout = graph.Layout(title=graph.layout.Title(text=title))) \n",
        "    for i in range(len(all_history)):\n",
        "        fig.add_trace(graph.Scatter(x = all_history[i][\"x\"], \n",
        "                                    y = all_history[i][\"y\"],\n",
        "                                    name = all_history[i][\"legend\"])) \n",
        "    if log: fig.update_xaxes(type=\"log\")\n",
        "    fig.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}